# Recommended experimental design following SOTA practices
#
# OVERALL COST SUMMARY:
# =====================
# Total experiments across all stages: 81 + 35 + 15 + 36 + 36 = 203 runs
# Total estimated time: 92-141 hours (~3.8-5.9 days of continuous GPU time)
# Peak GPU memory required: 20GB (stage 4 with 16 particles), 15GB (stage 5 with 32 views)
# Total storage needed: ~492-839GB across all experiments
# 
# RECOMMENDED EXECUTION ORDER:
# 1. Run exp_001 (coarse_hyperparameter_search) first - identify best method
# 2. Update exp_002 parameters based on exp_001 results 
# 3. Run exp_002 (fine_hyperparameter_search) - optimize best method
# 4. Update exp_003 parameters based on exp_002 best results
# 5. Run exp_003 (generalization_test) - test across prompts
# 6. Run exp_004 (efficiency_analysis) - computational analysis
# 7. Run exp_005 (multi_view_consistency_analysis) - optimize CLIP fidelity
#
# SLURM RESOURCE RECOMMENDATIONS:
# - Standard runs (exp_001-003): --mem=16G --gres=gpu:1 
# - High-memory runs (exp_004): --mem=24G --gres=gpu:1
# - Consider using --partition=gpgpuB for long jobs

# Stage 1: Coarse hyperparameter search
# COST ESTIMATE:
#   Total combinations: 3 × 3 × 3 × 3 = 81 runs
#   Time per run: ~25-35 min (1200 iters)
#   Total time: ~35-47 hours
#   GPU memory: ~8-12GB peak per run
#   Storage: ~2-3GB per run (81 runs ≈ 162-243GB total)
#   Recommended SLURM: --time=48:00:00 --mem=16G --gres=gpu:1
exp_001:
  name: coarse_hyperparameter_search
  method: grid
  sweep_parameters:
    repulsion_type: ['svgd', 'rlsd', 'wo']  # Main ablation
    feature_layer: [3, 6, 11]              # Architecture ablation
    lambda_repulsion: [1, 100, 1000, 10000]      # Wide range (log scale)
  
  fixed_parameters:
    seed: [42, 123, 456]                   # Multiple seeds for statistical significance
    prompts: ["a photo of a hamburger"]    # Single prompt for consistency
    num_particles: [4]                     # Keep fixed for fair comparison
    multi_view_type: ['average_views']
    num_views: [8]
    repulsion_tau: [0]
    lambda_sd: [1.0]
    kernel_type: ['rbf']
    iters: [1500]                          # ~25-35 min per run

  settings:
    visualize: true
    save_iid: false
    save_rendered_images_interval: 100
    save_multi_viewpoints_interval: 100
    metrics: true
    quantitative_metrics_interval: 25
    losses_interval: 25
    efficiency_interval: 25

# Stage 2: Fine-grained search around best coarse results
# COST ESTIMATE:
#   Total combinations: 1 × 1 × 7 × 5 = 35 runs
#   Time per run: ~30-45 min (1500 iters)
#   Total time: ~17-26 hours
#   GPU memory: ~8-12GB peak per run
#   Storage: ~3-4GB per run (35 runs ≈ 105-140GB total)
#   Recommended SLURM: --time=30:00:00 --mem=16G --gres=gpu:1
exp_002:
  name: fine_hyperparameter_search
  method: grid
  sweep_parameters:
    # Focus on best performing method from exp_001
    repulsion_type: ['svgd']  # Assume SVGD performed best
    feature_layer: [6]        # Assume layer 6 performed best
    lambda_repulsion: [500, 700, 900, 1000, 1100, 1200, 1300, 1400, 1500]  # Fine-grained around best value
  
  fixed_parameters:
    seed: [42]  # More seeds for final evaluation
    prompts: ["a photo of a hamburger"]
    num_particles: [4]
    multi_view_type: ['average_views']
    num_views: [8]
    repulsion_tau: [0]
    lambda_sd: [1.0]
    kernel_type: ['rbf']
    iters: [1500]                    # ~30-45 min per run (longer training)

  settings:
    visualize: true
    save_iid: false
    save_rendered_images_interval: 100
    save_multi_viewpoints_interval: 300
    metrics: true
    quantitative_metrics_interval: 25
    losses_interval: 10
    efficiency_interval: 10

# Stage 6: Visaulize features from different layers
# COST ESTIMATE:
#   Total combinations: 3 × 4 × 3 = 36 runs
#   Time per run: ~30-45 min (1500 iters)
#   Total time: ~18-27 hours
#   GPU memory: ~8-15GB peak (varies with num_views)
#   Storage: ~3-5GB per run (36 runs ≈ 108-180GB total)
#   Recommended SLURM: --time=30:00:00 --mem=16G --gres=gpu:1
exp_002:
  name: visualize_features_from_different_layers
  method: grid
  sweep_parameters:
    feature_layer: [3, 6, 11]
  
  fixed_parameters:
    seed: [42]
    prompts: ["a photo of a hamburger"]
    repulsion_type: ['svgd', 'rlsd', 'wo']
    lambda_repulsion: [700]
    num_particles: [8]
    multi_view_type: ['average_views']
    num_views: [8]
    repulsion_tau: [0]
    lambda_sd: [1.0]
    kernel_type: ['rbf']
    iters: [1500]

  settings:
    visualize: true
    save_iid: false
    save_rep_features: true # save features from different layers
    save_rep_features_interval: 100
    save_rendered_images_interval: 100
    save_multi_viewpoints_interval: 100
    metrics: true
    quantitative_metrics_interval: 25
    losses_interval: 25
    efficiency_interval: 25

# Stage 3: Multi-prompt generalization test
# COST ESTIMATE:
#   Total combinations: 5 × 3 = 15 runs
#   Time per run: ~30-45 min (1500 iters)
#   Total time: ~7.5-11 hours
#   GPU memory: ~8-12GB peak per run
#   Storage: ~3-4GB per run (15 runs ≈ 45-60GB total)
#   Recommended SLURM: --time=12:00:00 --mem=16G --gres=gpu:1
exp_003:
  name: generalization_test
  method: grid
  sweep_parameters:
    prompts: [
      "a photo of a hamburger",
      "a photo of a sports car", 
      "a photo of a wooden chair",
      "a photo of a cute cat",
      "a photo of a modern house"
    ]
  
  fixed_parameters:
    # Use best hyperparameters from exp_002
    seed: [42, 123, 456]
    repulsion_type: ['svgd']  # Best method
    feature_layer: [6]        # Best layer
    lambda_repulsion: [100]   # Best lambda (example)
    num_particles: [4]
    multi_view_type: ['average_views']
    num_views: [8]
    repulsion_tau: [0]
    lambda_sd: [1.0]
    kernel_type: ['rbf']
    iters: [1500]             # ~30-45 min per run

  settings:
    visualize: true
    save_iid: false
    save_rendered_images_interval: 100
    save_multi_viewpoints_interval: 300
    metrics: true
    quantitative_metrics_interval: 25
    losses_interval: 10
    efficiency_interval: 10

# Stage 4: Computational efficiency analysis
# COST ESTIMATE:
#   Total combinations: 4 × 3 × 3 = 36 runs
#   Time per run: ~12-60 min (varies: 600 iters=12min, 1200 iters=25min, 2400 iters=50min)
#   Total time: ~15-30 hours (varied due to different iter counts)
#   GPU memory: ~6-20GB peak (varies with num_particles: 2=6GB, 4=8GB, 8=12GB, 16=20GB)
#   Storage: ~1-6GB per run (varies with particles/iters, 36 runs ≈ 72-216GB total)
#   Recommended SLURM: --time=32:00:00 --mem=24G --gres=gpu:1 (higher mem for 16 particles)
exp_004:
  name: efficiency_analysis
  method: grid
  sweep_parameters:
    num_particles: [2, 4, 8, 16]      # Memory: 6GB, 8GB, 12GB, 20GB respectively
    iters: [600, 1200, 2400]          # Time: 12min, 25min, 50min respectively
  
  fixed_parameters:
    seed: [42, 123, 456]
    prompts: ["a photo of a hamburger"]
    # Use best hyperparameters from previous experiments
    repulsion_type: ['svgd']
    feature_layer: [6]
    lambda_repulsion: [100]
    multi_view_type: ['average_views']
    num_views: [8]
    repulsion_tau: [0]
    lambda_sd: [1.0]
    kernel_type: ['rbf']

  settings:
    visualize: true
    save_iid: false
    save_rendered_images_interval: 200
    save_multi_viewpoints_interval: 500
    metrics: true
    quantitative_metrics_interval: 50
    losses_interval: 10
    efficiency_interval: 5  # More frequent efficiency tracking

# Stage 5: Multi-view consistency and CLIP fidelity analysis
# COST ESTIMATE:
#   Total combinations: 3 × 4 × 3 = 36 runs
#   Time per run: ~30-45 min (1500 iters)
#   Total time: ~18-27 hours
#   GPU memory: ~8-15GB peak (varies with num_views)
#   Storage: ~3-5GB per run (36 runs ≈ 108-180GB total)
#   Recommended SLURM: --time=30:00:00 --mem=16G --gres=gpu:1
exp_005:
  name: multi_view_consistency_analysis
  method: grid
  sweep_parameters:
    multi_view_type: ['average_views']  # CLIP fidelity strategies
    num_views: [4, 8, 16, 32]              # Multi-view consistency analysis
  
  fixed_parameters:
    seed: [42, 123, 456]                   # Multiple seeds for statistical significance
    prompts: ["a photo of a hamburger"]    # Fixed prompt for controlled comparison
    # Use best hyperparameters from previous experiments
    repulsion_type: ['svgd']               # Best method from exp_001
    feature_layer: [6]                     # Best layer from exp_001  
    lambda_repulsion: [100]                # Best lambda from exp_002
    num_particles: [4]                     # Standard particle count
    repulsion_tau: [0]
    lambda_sd: [1.0]
    kernel_type: ['rbf']
    iters: [1500]                          # ~30-45 min per run

  settings:
    visualize: true
    save_iid: false
    save_rendered_images_interval: 100     # Frequent saves for visual analysis
    save_multi_viewpoints_interval: 250    # More frequent multi-view saves
    metrics: true
    quantitative_metrics_interval: 25      # Frequent metrics for consistency analysis
    losses_interval: 10
    efficiency_interval: 10